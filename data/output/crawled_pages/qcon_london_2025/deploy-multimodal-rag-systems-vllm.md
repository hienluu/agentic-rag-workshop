{
    "page_url": "https://qconlondon.com/presentation/apr2025/deploy-multimodal-rag-systems-vllm",
    "page_title": "QCon London 2025 | Deploy MultiModal RAG Systems with vLLM",
    "page_content": "Your choice regarding cookies on this site\nWe use cookies to optimise site functionality and give you the best possible experience.\nI AcceptI RejectCookie Preferences\nYou are viewing content from a past/completed conference.\n# Deploy MultiModal RAG Systems with vLLM\n### Summary\nDisclaimer: This summary has been generated by AI. It is experimental, and feedback is welcomed. Please reach out to info@qconlondon.com with any comments or concerns. \nIn the presentation \"Deploy MultiModal RAG Systems with vLLM,\" the speaker Stephen Batifol discusses the implementation and deployment of multimodal Retrieval-Augmented Generation (RAG) systems leveraging vLLM. This presentation aims to explore and address challenges in building RAG systems that process diverse data types such as images, audio, and documents, moving beyond text-only implementations.\n**Key Points:**\n  * _Introduction to Vector Databases:_ Vector databases are essential for managing and retrieving embeddings from multimodal inputs. These databases facilitate the transformation of diverse data into embeddings stored for fast access and retrieval.\n  * _Importance of Multimodal Systems:_ Typical RAG systems focus on text; however, real-world applications often involve multiple data formats. Examples include integrating sensor data with visual feedback in robotics or correlating manufacturing logs with images in automated inspection systems.\n  * _Architecture Overview:_ The architecture involves using tools like Milvus for vector databases, vLLM for model inference, and Pixel for processing multimodal inputs. These components help process and retrieve relevant information from embedded data.\n  * _Performance Optimization:_ Discussion on optimization techniques such as parallelism and dynamic batching. These methods improve the latency and throughput of deployed systems, enabling efficient real-time processing.\n  * _Evaluation and Retrieval:_ Evaluation of RAG systems is crucial, emphasizing the importance of proper context retrieval to ensure the effectiveness of the model. The speaker highlighted the necessity of accurately measuring system performance and refining retrieval mechanisms to prevent irrelevant or misleading results.\n\n\nThe session additionally includes a live demonstration showcasing a practical application where images and text queries are processed together. This provides insights into deploying RAG systems on self-hosted infrastructure to enhance control, privacy, and reduce API costs.\nThis is the end of the AI-generated content.\n* * *\n### Abstract\nWhile text-based RAG systems have been everywhere in the last year and a half, there is so much more than text data. Images, audio, and documents often need to be processed together to provide meaningful insights, yet most RAG implementations focus solely on text. Think automated visual inspection systems understanding both manufacturing logs and production line images, or robotics systems correlating sensor data with visual feedback. These multimodal scenarios demand RAG systems that go beyond text-only processing.\nIn this talk, we'll talk about how one can build a MultiModal RAG system that helps solve this problem. We'll explore the architecture that makes it possible to run such a system and demonstrate how to build one using Milvus, LlamaIndex, and vLLM for deploying open-source LLMs on your own infrastructure.\nThrough a live demo, we'll showcase a real-world application processing both images and text queries. Whether you're looking to reduce API costs, maintain data privacy, or simply gain more control over your AI infrastructure, this session will provide you with actionable insights to implement MultiModal RAG in your organization.\n## Interview:\n### What is the focus of your work?\nI focus on GenAI usage, going from simple RAG systems to full Agentic ones. I also highlight how search works at Scale. I am a big open source fan so most of my work is focused around that. \n### What\u2019s the motivation for your talk?\nTo showcase that you can deploy open source apps that can be very good. The idea is to showcase to people that they can be in control and not dependent on closed source systems.\n### Who is your talk for?\nPeople interested in moving from OpenAI and they want to control their GenAI stack. Also people interested in Multimodality.\n### What do you want someone to walk away with from your presentation?\nLearn how specific open source tools like vLLMs can match or exceed proprietary solutions while giving you full control over your AI stack and SLAs.\n### What do you think is the next big disruption in software?\nI believe that the combination of open source AI models and rapid development tools will enable more customized, sovereign AI solutions.\nPeople will have their own unique version of their software and likely not rely as much on the typical apps we used to have. \n* * *\n### Speaker\n#### Stephen Batifol\nDeveloper Advocate @Zilliz, Founding Member of the MLOps Community Berlin, Previously Machine Learning Engineer @Wolt, and Data Scientist @Brevo\nStephen Batifol is a Developer Advocate at Zilliz. He previously worked as a Machine Learning Engineer at Wolt, where he created and worked on the ML Platform, and previously as a Data Scientist at Brevo. Stephen studied Computer Science and Artificial Intelligence.\nHe is a founding member of the MLOps.community Berlin group, where he organizes Meetups and hackathons. He enjoys boxing and surfing.\nRead more\n#####  Find Stephen Batifol at: \n  * \n\n#### Speaker\n##### Stephen Batifol\nDeveloper Advocate @Zilliz, Founding Member of the MLOps Community Berlin, Previously Machine Learning Engineer @Wolt, and Data Scientist @Brevo\n#### Date\nTuesday Apr 8 / 10:35AM BST ( 50 minutes )\n#### Location\nWhittle (3rd Fl.)\n#### Track\nAI and ML for Software Engineers: Foundational Insights\n#### Topics\nAI/ML vLLM K8s\n#### Share\nShare Share\n## From the same track\nSession AI/ML\n### How to Unlock Insights and Enable Discovery Within Petabytes of Autonomous Driving Data\nTuesday Apr 8 / 05:05PM BST\nFor autonomous vehicle companies, finding valuable insights within millions of hours of video data is essential yet challenging. \nKyra Mozley\nML Engineer @Wayve, Previously Security & AI PhD Candidate @Royal Holloway University\nHow to Unlock Insights and Enable Discovery Within Petabytes of Autonomous Driving Data\nSession AI/ML\n### AI for Food Image Generation in Production: How & Why\nTuesday Apr 8 / 01:35PM BST\nIn this talk, we will conduct a technical overview of a client-facing Food Image Generation solution developed at Delivery Hero. \nIaroslav Amerkhanov\nSenior Data Scientist @Delivery Hero, Founder of T4lky, Creator & Host of EPAM Podcast, Speaker\nAI for Food Image Generation in Production: How & Why\nSession AI/ML\n### Foundation Models for Ranking: Challenges, Successes, and Lessons Learned\nTuesday Apr 8 / 02:45PM BST\nRecommender systems are an integral part of most products nowadays and are often a key driver of discovery for users of the product. \nMoumita Bhattacharya\nSenior Research Scientist @Netflix, Previously @Etsy\nFoundation Models for Ranking: Challenges, Successes, and Lessons Learned\nSession AI/ML\n### Building Embedding Models for Large-Scale Real-World Applications\nTuesday Apr 8 / 03:55PM BST\nEmbedding models are at the core of search, recommendation, and retrieval-augmented generation (RAG) systems, transforming data into meaningful representations. \nSahil Dua\nSenior Software Engineer, Machine Learning @Google, Stanford AI, Co-Author of \u201cThe Kubernetes Workshop\u201d, Open-Source Enthusiast \nBuilding Embedding Models for Large-Scale Real-World Applications\nSession\n### Unconference: AI and ML for Software Engineers\nTuesday Apr 8 / 11:45AM BST\nUnconference: AI and ML for Software Engineers\n"
}