{
    "page_url": "https://qconlondon.com/presentation/apr2025/achieving-precision-ai-retrieving-right-data-using-ai-agents",
    "page_title": "QCon London 2025 | Achieving Precision in AI: Retrieving the Right Data Using AI Agents",
    "page_content": "Your choice regarding cookies on this site\nWe use cookies to optimise site functionality and give you the best possible experience.\nI AcceptI RejectCookie Preferences\nYou are viewing content from a past/completed conference.\n# Achieving Precision in AI: Retrieving the Right Data Using AI Agents\n### Summary\nDisclaimer: This summary has been generated by AI. It is experimental, and feedback is welcomed. Please reach out to info@qconlondon.com with any comments or concerns. \n**Presentation Title:** Achieving Precision in AI: Retrieving the Right Data Using AI Agents\n**Speaker:** Adi Polak\n**Overview:** This presentation focuses on improving the precision of AI systems, especially in the context of Retrieval-Augmented Generation (RAG) and the development of agentic systems to manage data retrieval and processing with greater accuracy.\n**Key Points:**\n  * _Challenges of Precision:_ Emphasizes the difficulty in achieving precision in AI systems, which is crucial for transitioning from prototypes to production-grade AI systems.\n  * _Agentic RAG:_ Introduces agentic RAG as an advanced AI architecture that improves precision by segmenting data storage and retrieval for training vs. inference. This approach reduces latency and costs.\n  * _Component Needs:_ Highlights the need for applications that interact with LLMs while utilizing short-term and long-term memory, caching capabilities, and intelligent feedback loops.\n  * _Integration and Optimization:_ Discusses the integration of multiple data retrieval methods, including term search and similarity search, to enhance data accuracy and retrieval speed.\n  * _Agent Functionality:_ Describes the functionalities of AI agents, including planning, acting, and use of feedback loops to improve decision-making processes.\n  * _Precision Metrics:_ Emphasizes the importance of specific metrics for evaluating the precision of AI models, including metrics for hallucination rates and continuous feedback loops.\n\n\n**Conclusion:** The presentation underscores the necessity of precise data retrieval for the efficacy of AI systems and highlights agentic RAG as a pioneering approach to enhance accuracy while minimizing resource use and operational costs.\nThis is the end of the AI-generated content.\n* * *\n### Abstract\nIn the race to harness the power of generative AI, organizations are discovering a hidden challenge: precision. Models are only as effective as the data they access, yet most approaches to Retrieval-Augmented Generation (RAG) lack the dedicated, fine-tuned pipelines needed to ensure the right information is delivered at the right time.\nToday, most RAG systems pull from vast, generalized data lakes, leading to noisy outputs and frustrating inefficiencies. The result? Wasted resources, inconsistent responses, and missed opportunities for real-time decision-making. But what if you could create an AI system that doesn\u2019t just retrieve data\u2014but understands its context, delivering precise, actionable insights in milliseconds?\nThis is where agenticRAG comes into play\u2014a breakthrough in AI architecture that pairs dedicated retrieval pipelines with intelligent agents to deliver pinpoint accuracy. By segmenting your data storage and retrieval processes specifically for training vs. inference, you can achieve hyper-focused precision while dramatically reducing latency and costs.  \nImagine an AI system that knows exactly what data it needs and how to get it with zero lag\u2014a system that\u2019s tuned to perform like a well-trained expert in your domain.\nCurious to discover how you can optimize your AI applications for laser-focused accuracy? Join me as to learn more about AgenticRAG and fine tuning your models.\n* * *\n### Speaker\n#### Adi Polak\nDirector, Advocacy and Developer Experience Engineering @Confluent, Author of \"Scaling Machine Learning with Spark\" and \"High Performance Spark 2nd Edition\"\nAdi is an experienced Software Engineer and people manager. She has worked with data and machine learning for operations and analytics for over a decade. As a data practitioner, she developed algorithms to solve real-world problems using machine learning techniques while leveraging expertise in distributed large-scale systems to build machine learning and data streaming pipelines. As a manager, Adi builds high-performance teams focused on trust, excellence, and ownership.\nAdi has taught thousands of students how to scale machine learning systems and is the author of the successful book Scaling Machine Learning with Spark and High Performance Spark 2nd Edition.\nRead more\n#### Speaker\n##### Adi Polak\nDirector, Advocacy and Developer Experience Engineering @Confluent, Author of \"Scaling Machine Learning with Spark\" and \"High Performance Spark 2nd Edition\"\n#### Date\nWednesday Apr 9 / 11:45AM BST ( 50 minutes )\n#### Location\nWhittle (3rd Fl.)\n#### Track\nModern Data Architectures\n#### Topics\nAI/ML architecture Data Systems AI\n#### Share\nShare Share\n## From the same track\nSession Data Architecture\n### Reliable Data Flows and Scalable Platforms: Tackling Key Data Challenges\nWednesday Apr 9 / 10:35AM BST\nThere are a few common and mostly well-known challenges when architecting for data. For example, many data teams struggle to move data in a stable and reliable way from operational systems to analytics systems. \nMatthias Niehoff\nHead of Data and Data Architecture @codecentric AG, iSAQB Certified Professional for Software Architecture\nReliable Data Flows and Scalable Platforms: Tackling Key Data Challenges\nSession Data Architecture\n### Beyond the Warehouse: Why BigQuery Alone Won\u2019t Solve Your Data Problems\nWednesday Apr 9 / 03:55PM BST\nMany organizations mistake the adoption of a data warehouse, like BigQuery, as the golden ticket to solving all their data challenges. But without a robust data strategy and architecture, you\u2019re simply shifting chaos into the cloud. \nSarah Usher\nData & Backend Engineer, Community Director, Mentor\nBeyond the Warehouse: Why BigQuery Alone Won\u2019t Solve Your Data Problems\nSession AI/ML\n### The Data Backbone of LLM Systems\nWednesday Apr 9 / 02:45PM BST\nAny LLM application has four dimensions you must carefully engineer: the code, data, models and prompts. Each dimension influences the other. That's why you must learn how to track and manage each. The trick is that every dimension has particularities requiring unique strategies and tooling. \nPaul Iusztin\nSenior ML/AI Engineer, MLOps, Founder @Decoding ML\nThe Data Backbone of LLM Systems\nSession\n### Panel: Modern Data Architectures\nWednesday Apr 9 / 01:35PM BST\nPanel: Modern Data Architectures\n"
}