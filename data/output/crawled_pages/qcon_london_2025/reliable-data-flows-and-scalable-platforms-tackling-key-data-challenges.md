{
    "page_url": "https://qconlondon.com/presentation/apr2025/reliable-data-flows-and-scalable-platforms-tackling-key-data-challenges",
    "page_title": "QCon London 2025 | Reliable Data Flows and Scalable Platforms: Tackling Key Data Challenges",
    "page_content": "Your choice regarding cookies on this site\nWe use cookies to optimise site functionality and give you the best possible experience.\nI AcceptI RejectCookie Preferences\nYou are viewing content from a past/completed conference.\n# Reliable Data Flows and Scalable Platforms: Tackling Key Data Challenges\n### Summary\nDisclaimer: This summary has been generated by AI. It is experimental, and feedback is welcomed. Please reach out to info@qconlondon.com with any comments or concerns. \nThe presentation titled \"Reliable Data Flows and Scalable Platforms: Tackling Key Data Challenges\" was delivered by Matthias Niehoff. It focuses on addressing common challenges faced by data teams in designing and implementing data architectures.\n**Key Themes and Concepts:**\n  * The challenge of moving data reliably from operational systems to analytics systems while managing complex infrastructure landscapes.\n  * Highlighting the importance of reliable and scalable data platforms and the costs associated with them.\n  * Emphasis on building platforms that support data flows from operational data sources to analytics outcomes.\n\n\n**Solutions and Strategies:**\n  * Integration of data pipelines and consistent schema contracts to simplify operations and improve reliability.\n  * Approaches for reducing complexity which in turn reduces the cost and error potential.\n  * Use of document and relational databases to manage and visualize data effectively.\n  * Adopting software engineering practices such as separation of environments and contract definitions between applications and data solutions.\n\n\n**Conclusion:**\nThe talk underscores the importance of simplifying architectural designs to minimize complexity and support data-driven decision-making processes effectively, thus enhancing the scalability and performance of data platforms.\nThis is the end of the AI-generated content.\n* * *\n### Abstract\nThere are a few common and mostly well-known challenges when architecting for data. For example, many data teams struggle to move data in a stable and reliable way from operational systems to analytics systems. At the same time, they must manage complex and often costly infrastructure landscapes. These issues hinder companies from effectively leveraging their data for business purposes.\nDrawing from real-world experience, this presentation will explore how we address these challenges by building reliable and scalable data platforms with reasonable costs. It will also cover solutions to help operational teams provide their data and to observe the flow towards analytics systems. In addition to discussing architectures and design considerations the presentation will also highlight tools and techniques used to implement these platforms.\n## Interview:\n### What is the focus of your work?\nIn one sentence, I would say: availability of data for analytical use, regardless of whether it is a dashboard, machine learning or GenAI. In addition to data platforms and infrastructures, this often includes the transfer of data from operational systems to analytical systems, which is often still treated rather neglected and offers many pitfalls.\n### What\u2019s the motivation for your talk?\nMany data initiatives fail to obtain reliable and consistent data. There are many different approaches to solving this problem, some of which are borrowed from well-known software engineering best practices. I would like to present a few of these solutions and, above all, show how we have implemented them in practice. Without the costs exploding.\n### Who is your talk for?\nOf course, the talk is primarily aimed at data architects and data engineers. But I also invite all software engineers to take a look at the topic. They too are increasingly coming into contact with the supply of data and can make a big difference. Plus knowing about whats possible there could make their lives easier.\n### What do you want someone to walk away with from your presentation?\nIdeas on how I can provide data better in the sense of more stable and correct. Tangible possibilities of which tools and processes can be used to implement this. Without sinking into complexity and ultimately costs.\n### What do you think is the next big disruption in software?\nI believe that one of the big issues will be the reduction of complexity. Nowadays, modern IT landscapes are a patchwork of barely comprehensible building blocks that are somehow held together. In addition to maintenance, this also makes it difficult to test and implement new features in order to quickly adapt to the market. In this context, automation (e.g. through AI) will play a major role on both the business and technical side.\n### What was one interesting thing that you learned from a previous QCon?\nI was already familiar with duckdb, but in 2023 I understood in Hannes M\u00fchleisen's presentation what possibilities the technology offers and what diverse applications it enables. That was the first time I was able to imagine the changes in data architectures that are possible thanks to the new, lean processing frameworks.\n* * *\n### Speaker\n#### Matthias Niehoff\nHead of Data and Data Architecture @codecentric AG, iSAQB Certified Professional for Software Architecture\nMatthias Niehoff works as Head of Data and Data Architect for codecentric AG and supports customers in the design and implementation of data architectures. His focus is on the necessary infrastructure and organization to help data and ML projects succeed.\nRead more\n#####  Find Matthias Niehoff at: \n  * \n\n#### Speaker\n##### Matthias Niehoff\nHead of Data and Data Architecture @codecentric AG, iSAQB Certified Professional for Software Architecture\n#### Date\nWednesday Apr 9 / 10:35AM BST ( 50 minutes )\n#### Location\nWhittle (3rd Fl.)\n#### Track\nModern Data Architectures\n#### Topics\nData Architecture Data engineering Observability\n#### Share\nShare Share\n## From the same track\nSession AI/ML\n### Achieving Precision in AI: Retrieving the Right Data Using AI Agents\nWednesday Apr 9 / 11:45AM BST\nIn the race to harness the power of generative AI, organizations are discovering a hidden challenge: precision. \nAdi Polak\nDirector, Advocacy and Developer Experience Engineering @Confluent, Author of \"Scaling Machine Learning with Spark\" and \"High Performance Spark 2nd Edition\"\nAchieving Precision in AI: Retrieving the Right Data Using AI Agents\nSession Data Architecture\n### Beyond the Warehouse: Why BigQuery Alone Won\u2019t Solve Your Data Problems\nWednesday Apr 9 / 03:55PM BST\nMany organizations mistake the adoption of a data warehouse, like BigQuery, as the golden ticket to solving all their data challenges. But without a robust data strategy and architecture, you\u2019re simply shifting chaos into the cloud. \nSarah Usher\nData & Backend Engineer, Community Director, Mentor\nBeyond the Warehouse: Why BigQuery Alone Won\u2019t Solve Your Data Problems\nSession AI/ML\n### The Data Backbone of LLM Systems\nWednesday Apr 9 / 02:45PM BST\nAny LLM application has four dimensions you must carefully engineer: the code, data, models and prompts. Each dimension influences the other. That's why you must learn how to track and manage each. The trick is that every dimension has particularities requiring unique strategies and tooling. \nPaul Iusztin\nSenior ML/AI Engineer, MLOps, Founder @Decoding ML\nThe Data Backbone of LLM Systems\nSession\n### Panel: Modern Data Architectures\nWednesday Apr 9 / 01:35PM BST\nPanel: Modern Data Architectures\n"
}