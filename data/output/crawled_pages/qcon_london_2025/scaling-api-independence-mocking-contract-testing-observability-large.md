{
    "page_url": "https://qconlondon.com/presentation/apr2025/scaling-api-independence-mocking-contract-testing-observability-large",
    "page_title": "QCon London 2025 | Scaling API Independence: Mocking, Contract Testing & Observability in Large Microservices Environments",
    "page_content": "Your choice regarding cookies on this site\nWe use cookies to optimise site functionality and give you the best possible experience.\nI AcceptI RejectCookie Preferences\nYou are viewing content from a past/completed conference.\n# Scaling API Independence: Mocking, Contract Testing & Observability in Large Microservices Environments\n### Summary\nDisclaimer: This summary has been generated by AI. It is experimental, and feedback is welcomed. Please reach out to info@qconlondon.com with any comments or concerns. \nThe presentation titled \"Scaling API Independence: Mocking, Contract Testing & Observability in Large Microservices Environments\" by Tom Akehurst focuses on addressing challenges in microservices environments through mocking, contract testing, and observability.\n**Key Topics Discussed:**\n  * **Microservices Challenges:** Despite promises of independence, microservices often face issues like broken environments and dependency on other team APIs. These challenges hinder productivity and development speed.\n  * **Mocking and API Simulation:** Mocking helps decouple systems, but maintaining realistic mocks across thousands of APIs is challenging. Mock APIs might drift from real counterparts; hence, continuous validation is essential.\n  * **Contract Testing:** Contracts provide a syntactic description of APIs but lack behavioral context. Combining contracts with observations and simulations helps validate and ensure API reliability.\n  * **API Observability:** Observability involves actively and passively capturing API interactions to understand system behavior. Tools like EBPF and service mesh facilitate these processes without breaching encryption.\n  * **Productivity through AI:** AI and Large Language Models (LLMs) can enhance API simulations by generating and refining mock data, serving as productivity levers when paired with existing contract testing infrastructure.\n\n\n**Conclusion:** The integration of these techniques\u2014mocking, contract testing, and observability\u2014enhances productivity and reliability within large microservices environments. They help in managing API complexities and improving the independence of service teams.\nThis is the end of the AI-generated content.\n* * *\n### Abstract\nMicroservices promise faster deployments and team autonomy. In reality, engineers are often blocked waiting for APIs, dealing with broken sandboxes, or wrangling test environments.\nMocking helps decouple dependencies and teams - but at the scale of 1,000+ internal APIs, maintaining realistic, reliable mocks is a challenge of its own. How do you ensure contract alignment? How do you keep mocks up-to-date without excessive maintenance?\nIn this talk, we'll explore new ways to combine mocking, contract testing, and traffic observation to support fast-flowing development and testing.\n## Interview:\n### What is the focus of your work?\nBuilding tools to help organisations who depend heavily on APIs develop and test more productively.\n### What\u2019s the motivation for your talk?\nI encounter many engineering orgs for whom the promise of microservices - decoupled teams shipping independently - isn't being realized. They're stuck firefighting flakey dependencies and debugging spurious test failures in integrated environments or waiting entire weekends for test runs to complete. In my view this is largely due to some limiting assumptions and beliefs, particularly about mocking/simulation of APIs and how this can be done effectively at scale.\n### Who is your talk for?\nSenior engineers/tech leads, engineering managers, senior QAs, QA managers\n### What do you want someone to walk away with from your presentation?\nWith increased confidence that API simulation can be a core pillar of their dev and test strategy, and some new ideas about how to achieve this in complex engineering organizations.\n### What do you think is the next big disruption in software?\nIt's hard to bet against AI coding tools at the moment.\n### What was one interesting thing that you learned from a previous QCon?\nIn 2013 Damian Conway's presentations were both unforgettable - the showmanship and craft involved in building a latin code interpreter just to present was incredibly impressive, and his second talk on presentation technique is still the one I refer back to the most.\n* * *\n### Speaker\n#### Tom Akehurst\nCTO and Co-Founder @WireMock, 20+ Years Building Enterprise Systems\nTom is a career software developer who\u2019s spent over 20 years building enterprise systems, primarily as a backend Java/JVM developer but with dabblings in infrastructure/DevOps, web development and performance engineering. He\u2019s spent more than half of that time thinking about how to develop and test networked services more productively and is the creator of the WireMock open source API mocking tool. Lately he\u2019s also the CTO and co-founder of WireMock, Inc.\nRead more\n#####  Find Tom Akehurst at: \n  *   * \n\n#### Speaker\n##### Tom Akehurst\nCTO and Co-Founder @WireMock, 20+ Years Building Enterprise Systems\n#### Date\nWednesday Apr 9 / 01:35PM BST ( 50 minutes )\n#### Location\nWhittle (3rd Fl.)\n#### Track\nConnecting Systems: APIs, Protocols, Observability\n#### Topics\nAPIs mocking test automation contract testing Microservices\n#### Share\nShare Share\n## From the same track\nSession resiliency\n### Timeouts, Retries and Idempotency In Distributed Systems\nWednesday Apr 9 / 10:35AM BST\nThe definition of insanity is doing the same thing over and over again\u201d - this quote attributed to Einstein warns us of the danger of magical thinking, hoping that trying something just one more time will achieve success when before we failed. But is this really insanity? \nSam Newman\nMicroservice, Cloud, CI/CD Expert, Author of \"Building Microservices\" and \"Monolith to Microservices\", 20+ Years Experience as a Developer\nTimeouts, Retries and Idempotency In Distributed Systems\nSession architecture\n### Platforms for Secure API Connectivity With Architecture as Code\nWednesday Apr 9 / 03:55PM BST\nAs microservices and complex platforms become the standard, ensuring secure connectivity while maintaining a smooth developer experience is a significant challenge. Traditional security models often introduce friction, slowing down innovation and deployment. \nJim Gough\nDistinguished Engineer, API Platform Lead Architect @Morgan Stanley, Co-Author of Optimizing Java\nPlatforms for Secure API Connectivity With Architecture as Code\nSession\n### From Dashboard Soup to Observability Lasagna: Building Better Layers\nWednesday Apr 9 / 02:45PM BST\nLet's be honest - observability can suck. Ever feel like you're swimming in dashboard soup? You know the feeling: tons of single-use dashboards, building new ones during every incident only to lose them in the chaos, and spending ages creating visualizations that no one ever looks at again. \nMartha Lambert\nProduct Engineer @incident.io, Building Reliable and Observable Systems\nFrom Dashboard Soup to Observability Lasagna: Building Better Layers\nSession architecture\n### From Confusion to Clarity: Advanced Observability Strategies for Media Workflows at Netflix\nWednesday Apr 9 / 11:45AM BST\nManaging media workflows at the Netflix scale is both thrilling and daunting. With millions of workflow executions across hundreds of types and over 500 million CPU hours consumed quarterly, costs can skyrocket, and encoding issues can disrupt the streaming experience. \nSujana Sooreddy\nSoftware Engineer @Netflix - Building High Scale Observability Solutions\nNaveen Mareddy\nStaff Engineer @Netflix, 20+ years in Software Engineering, Creator of MediaInfra Meetup, Speaker, Mentor\nFrom Confusion to Clarity: Advanced Observability Strategies for Media Workflows at Netflix\n"
}