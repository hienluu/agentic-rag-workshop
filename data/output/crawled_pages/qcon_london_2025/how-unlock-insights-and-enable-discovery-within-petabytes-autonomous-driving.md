{
    "page_url": "https://qconlondon.com/presentation/apr2025/how-unlock-insights-and-enable-discovery-within-petabytes-autonomous-driving",
    "page_title": "QCon London 2025 | How to Unlock Insights and Enable Discovery Within Petabytes of Autonomous Driving Data",
    "page_content": "Your choice regarding cookies on this site\nWe use cookies to optimise site functionality and give you the best possible experience.\nI AcceptI RejectCookie Preferences\nYou are viewing content from a past/completed conference.\n# How to Unlock Insights and Enable Discovery Within Petabytes of Autonomous Driving Data\n### Summary\nDisclaimer: This summary has been generated by AI. It is experimental, and feedback is welcomed. Please reach out to info@qconlondon.com with any comments or concerns. \nThe presentation titled \"**How to Unlock Insights and Enable Discovery Within Petabytes of Autonomous Driving Data** \" by Kyra Mozley from Wayve discusses advanced methods for processing large-scale autonomous driving data.\nThe key points of the presentation are:\n  * **Foundation Models and Embeddings** : The use of foundation models and embeddings is emphasized for their ability to improve data discovery through scalable search tools, which enhance data labeling and retrieval efficiency.\n  * **Vision-Language Models (VLMs)** : VLMs are used to locate relevant driving scenarios, crucial for meeting safety standards and evaluating driving behaviors.\n  * **Edge Cases** : The identification and understanding of rare scenarios are highlighted as crucial due to their safety implications.\n  * **Technical Infrastructure** : The presentation covers the architecture behind the tools, including vector databases and Flyte workflows for scalable data processing.\n  * **Future Applications** : Future advancements such as on-device edge filtering are explored, aiming to reduce data storage by capturing only significant scenarios in real time.\n  * **Auto-labeling and Clustering** : Discussions on auto-labeling via foundation models and clustering for unsupervised labeling, enabling the discovery of patterns within data without manual intervention.\n  * **Embedding-based Search and RAG Techniques** : Introduces embedding-based search for scene retrieval and the refinement of this process using query generation and multi-query fusion techniques to enhance search accuracy.\n  * **Training Lightweight Classifiers** : Proposes the training of lightweight classifiers on embeddings for specific tasks without the need for extensive data sets.\n\n\nThis session provided a comprehensive overview of how embedding and foundation models revolutionize data handling in autonomous vehicle technology, offering a scalable approach to processing and evaluating large data sets with minimal manual intervention.\nThis is the end of the AI-generated content.\n* * *\n### Abstract\nFor autonomous vehicle companies, finding valuable insights within millions of hours of video data is essential yet challenging. This talk explores how we at Wayve are leveraging foundation models and embeddings to build scalable search tools that make data discovery faster and labeling more efficient.\nAttendees will learn how we leverage vision-language models (VLMs) to retrieve relevant scenarios at scale, which is invaluable for pinpointing scenes needed to meet safety standards or evaluate specific driving behaviors. By using embeddings, we can train classifiers to detect specific driving competencies. Through an active learning loop, we refine these classifiers, enabling them to label similar scenarios across the entire dataset with high efficiency. This embedding-based approach is both fast and scalable, and it also helps us spot \u201cbad data\u201d clusters, like images with droplets on the lens or scenes from test tracks.\nThe presentation will delve into the technical infrastructure behind these tools, from vector databases that enable rapid similarity search to Flyte workflows that orchestrate scalable processing across distributed systems. We\u2019ll also explore how query generation helps bridge the gap in positional awareness within text embeddings, allowing for more precise search across video datasets. Finally, the talk will close with a look toward future possibilities, such as on-device edge filtering, which would use embeddings to reduce storage costs by capturing only the most interesting scenarios in real time.\nDesigned for engineers and data scientists, this session provides a deep dive into the power of embeddings and VLMs for labeling and retrieving data at scale, making it possible to unlock insights and drive advancements in autonomous vehicle technology.\n## Interview:\n### What is the focus of your work?\nMy work focuses on building scalable pipelines for running perception models at scale (e.g., segmentation, cuboids, CLIP) and enhancing semantic search capabilities to enable users to search and retrieve relevant video data using natural language. I\u2019m also experimenting with the latest vision-language models (VLMs) for video understanding to address these challenges and perform data mining at scale.\n### What\u2019s the motivation for your talk?\nThe motivation stems from the growing need for efficient data mining and discovery in the autonomous vehicle industry. With petabytes of data collected from our fleet, we need to surface valuable insights across diverse teams\u2014from safety validation to offline evaluation\u2014while addressing specific challenges like dataset coverage, behavioural evaluation, and bad data removal. This talk aims to highlight how we leverage foundation models and embeddings to solve these challenges, showcasing how scalable search and retrieval tools can transform data understanding and accelerate innovation.\n### Who is your talk for?\nThis talk is designed for data scientists, machine learning engineers, and anyone working on video understanding, large-scale ML pipelines, search and retrieval, or autonomous driving technology. It\u2019s particularly relevant for teams dealing with vast datasets and looking to leverage open foundation models for smarter data processing and retrieval.\n### What do you want someone to walk away with from your presentation?\nI want attendees to leave with a clear understanding of how foundation models and embeddings can revolutionise data retrieval and labelling at scale. They\u2019ll learn how to design robust pipelines for large-scale data processing, use VLMs for scene retrieval, and apply learning loops to scale classifier training. Additionally, I hope to inspire ideas for exploring future possibilities, like on-device edge filtering, to optimise storage and processing costs.\n### What do you think is the next big disruption in software?\nThe next big disruption will likely come from the intersection of edge computing and foundation models. Real-time, on-device filtering and decision-making will enable systems to process and prioritise data at the source, dramatically reducing storage and compute costs while enhancing the speed and accuracy of downstream tasks. This will be particularly transformative in fields like autonomous vehicles, IoT, and video understanding, where real-time insights are critical.\n* * *\n### Speaker\n#### Kyra Mozley\nML Engineer @Wayve, Previously Security & AI PhD Candidate @Royal Holloway University\nMachine Learning Engineer @ Wayve. With a background in computer vision and deep learning, Kyra leads the development of tools that leverage foundation models and embeddings to efficiently process and understand vast amounts of autonomous vehicle driving data.\nRead more\n#####  Find Kyra Mozley at: \n  *   * \n\n#### Speaker\n##### Kyra Mozley\nML Engineer @Wayve, Previously Security & AI PhD Candidate @Royal Holloway University\n#### Date\nTuesday Apr 8 / 05:05PM BST ( 50 minutes )\n#### Location\nWhittle (3rd Fl.)\n#### Track\nAI and ML for Software Engineers: Foundational Insights\n#### Topics\nAI/ML Big Data compute at scale retrieval\n#### Slides\nSlides are not available\n#### Share\nShare Share\n## From the same track\nSession AI/ML\n### Deploy MultiModal RAG Systems with vLLM\nTuesday Apr 8 / 10:35AM BST\nWhile text-based RAG systems have been everywhere in the last year and a half, there is so much more than text data. Images, audio, and documents often need to be processed together to provide meaningful insights, yet most RAG implementations focus solely on text. \nStephen Batifol\nDeveloper Advocate @Zilliz, Founding Member of the MLOps Community Berlin, Previously Machine Learning Engineer @Wolt, and Data Scientist @Brevo\nDeploy MultiModal RAG Systems with vLLM\nSession AI/ML\n### AI for Food Image Generation in Production: How & Why\nTuesday Apr 8 / 01:35PM BST\nIn this talk, we will conduct a technical overview of a client-facing Food Image Generation solution developed at Delivery Hero. \nIaroslav Amerkhanov\nSenior Data Scientist @Delivery Hero, Founder of T4lky, Creator & Host of EPAM Podcast, Speaker\nAI for Food Image Generation in Production: How & Why\nSession AI/ML\n### Foundation Models for Ranking: Challenges, Successes, and Lessons Learned\nTuesday Apr 8 / 02:45PM BST\nRecommender systems are an integral part of most products nowadays and are often a key driver of discovery for users of the product. \nMoumita Bhattacharya\nSenior Research Scientist @Netflix, Previously @Etsy\nFoundation Models for Ranking: Challenges, Successes, and Lessons Learned\nSession AI/ML\n### Building Embedding Models for Large-Scale Real-World Applications\nTuesday Apr 8 / 03:55PM BST\nEmbedding models are at the core of search, recommendation, and retrieval-augmented generation (RAG) systems, transforming data into meaningful representations. \nSahil Dua\nSenior Software Engineer, Machine Learning @Google, Stanford AI, Co-Author of \u201cThe Kubernetes Workshop\u201d, Open-Source Enthusiast \nBuilding Embedding Models for Large-Scale Real-World Applications\nSession\n### Unconference: AI and ML for Software Engineers\nTuesday Apr 8 / 11:45AM BST\nUnconference: AI and ML for Software Engineers\n"
}