{
    "page_url": "https://qconlondon.com/presentation/apr2025/streaming-really-large-data-flink-and-fluss",
    "page_title": "QCon London 2025 | Streaming Really Large Data With Flink and Fluss",
    "page_content": "Your choice regarding cookies on this site\nWe use cookies to optimise site functionality and give you the best possible experience.\nI AcceptI RejectCookie Preferences\nYou are viewing content from a past/completed conference.\n# Streaming Really Large Data With Flink and Fluss\n### Abstract\nModern data workloads are pushing the limits of our streaming systems. Row-oriented streaming pipelines often send every event (and every field) over the network, even if consumers only need a fraction of that data. With data volumes skyrocketing in the AI era \u2013 where machine learning and real-time analytics consume more data than ever \u2013 these inefficiencies become a serious bottleneck. In this talk, we explore a new approach that treats all data as tables, seamlessly integrating streams as continuously updating tables to make streaming more efficient and scalable.\nWhat you'll learn:  \nFiltering Data at the Source: See how combining Apache Paimon with Apache Arrow (a columnar format) enables predicate pushdown and columnar storage. This allows data to be filtered and pruned at the source, significantly cutting down unnecessary network traffic (often by up to 50%). By avoiding the row-by-row transfer of irrelevant data, we reduce network overhead and boost throughput.\nUnified Batch and Stream Queries: Learn how treating streams as tables lets you run the same queries across static data lakes and live streaming data without special-case code. We\u2019ll discuss how Apache Arrow\u2019s in-memory columnar capabilities, integrated with Apache Paimon\u2019s table format, make it possible to query historical data and real-time events in a unified way. This streaming-lakehouse approach simplifies architecture by using one table-oriented model for both batch and streaming workloads.\nScalable Pipeline Architecture: Discover strategies for chaining multiple processing jobs (with engines like Apache Flink and even Apache Spark) while maintaining data integrity and low latency. We\u2019ll cover how the open-source FLUSS project serves as a real-time streaming storage layer that works hand-in-hand with Flink. Drawing on real-world use cases \u2013 including how Alibaba\u2019s platforms handle massive, continuous data streams \u2013 we\u2019ll illustrate how this architecture supports billions of events without compromising performance.  \nThis session is designed for data engineers and architects looking to build scalable, cost-effective data pipelines that blend streaming and batch processing. You\u2019ll come away with practical insights into why our data architectures must evolve to support AI-driven demand, and how reimagining streams as tables can simplify your stack while delivering substantial performance gains and cost savings.\n* * *\n### Speaker\n#### Ben Gamble\nField CTO @Ververica\nA long builder of AI powered games, simulations, and collaborative user experiences. Ben has previously built a global logistics company. Large scale online games and Augmented reality apps. Ben currently works to make fast data and AI a reality for everyone. He is the Field CTO of Ververica\nRead more\n#### Session Sponsored By\nThe Unified Streaming Data Platform powered by VERA, from the original creators of Apache Flink\u00ae\n#### Speaker\n##### Ben Gamble\nField CTO @Ververica\n#### Date\nTuesday Apr 8 / 02:45PM BST ( 50 minutes )\n#### Location\nWestminster (4th Fl.)\n#### Track\nSponsored Solution Track II\n#### Video\nVideo is not available\n#### Share\nShare Share\n## From the same track\nSession\n### Beyond Code: Building a Personal Brand To Boost Your Career\nTuesday Apr 8 / 01:35PM BST\nIn an increasingly competitive field, software expertise alone may not be enough to stand out and drive your career forward. \nRoland Meertens\nInfoQ Editor, Machine Learning Engineer @Wayve, Previously @Bumble Inc, @Annotell, and @Autonomous Intelligent Driving\nSteef-Jan Wiggers\nCloud Queue Lead Editor @InfoQ, Principal Consultant Cloud/DevOps @Team Rockstars IT\nBeyond Code: Building a Personal Brand To Boost Your Career\nSession\n### From Concept to Code: Navigating Agentic AI Services \nTuesday Apr 8 / 11:45AM BST\nThose who embrace agentic AI will reap the rewards. Building on the strategic insights from the first session (\u201cA Blueprint for Agentic AI Services\u201d), this presentation delves into the technical intricacies of harnessing agentic AI. \nAlan Klikic\nSenior Solutions Architect @Akka\nFrom Concept to Code: Navigating Agentic AI Services \nSession\n### Engineering Excellence at ING: Balance Autonomy with Standardization\nTuesday Apr 8 / 10:35AM BST\nING is committed to empowering its engineers to maximize their impact and create more value for customers. To achieve this, ING continuously seeks innovative ways to accelerate development and enhance productivity. \nDaniele Tonella\nCTO @ING\nEngineering Excellence at ING: Balance Autonomy with Standardization\nSession\n### AI-Enabled Delivery - ICSAET Cohort Only\nTuesday Apr 8 / 03:55PM BST\nOnly available to attendees with a \u201cConference (3 days) + Certification (half day)\u201d ticket.AI isn't just about fancy code completion anymore \u2013 it's shaking up the entire software lifecycle, from design to deployment and beyond! \ud83e\udd2f \nWes Reisz\nTechnical Principal @EqualExperts, ex-Thoughtworker & ex-VMWare, 16-Time QCon Chair, Creator/Co-host of The InfoQ Podcast\nAI-Enabled Delivery - ICSAET Cohort Only\nSession\n### Unlock Continuous Testing with AI Test Agent Workflows\nTuesday Apr 8 / 05:05PM BST\nThis talk will explore how agentic AI is revolutionizing continuous testing workflows, with a practical demonstration of Diffblue Cover's autonomous AI-powered unit testing capabilities. \nAnimesh Mishra\nSenior Solutions Engineer\nUnlock Continuous Testing with AI Test Agent Workflows\n"
}