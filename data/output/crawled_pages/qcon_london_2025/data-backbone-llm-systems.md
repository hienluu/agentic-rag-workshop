{
    "page_url": "https://qconlondon.com/presentation/apr2025/data-backbone-llm-systems",
    "page_title": "QCon London 2025 | The Data Backbone of LLM Systems",
    "page_content": "Your choice regarding cookies on this site\nWe use cookies to optimise site functionality and give you the best possible experience.\nI AcceptI RejectCookie Preferences\nYou are viewing content from a past/completed conference.\n# The Data Backbone of LLM Systems\n### Summary\nDisclaimer: This summary has been generated by AI. It is experimental, and feedback is welcomed. Please reach out to info@qconlondon.com with any comments or concerns. \n**Summary of 'The Data Backbone of LLM Systems' Presentation**\nThe presentation, given by Paul Iusztin, centers on the crucial role that data plays in constructing Large Language Model (LLM) systems. It highlights four key dimensions: code, data, models, and prompts, with a focus on the data aspect. Paul discusses strategies to manage and integrate these dimensions effectively using a framework for building LLM applications.\n**Key Topics Covered:**\n  * **LLM Application Dimensions:** Understanding the interdependent nature of code, data, models, and prompts, emphasizing the need for tailored strategies and tooling for each.\n  * **Data Engineering:** Exploration of data flows in the LLM system, focusing on pipelines for data features, storage solutions for data sharing, versioning, processing, and analysis needed for training and inference.\n  * **Fine-Tuning and Inference:** Detailed exploration of data access during LLM fine-tuning and inference, implemented through frameworks like Retrieval-Augmented Generation (RAG).\n\n\n**Use Cases:**\n  1. _LLM Twin: Your Digital AI Replica_ - A practical implementation from the LLM Engineer's Handbook.\n  2. _Second Brain AI Assistant_ - Demonstrated through an open-source course, providing insights into hands-on implementation.\n\n\n**Additional Insights:**\n  * **Data Pipelines:** Introduction to the feature/training/inference architecture pattern stressing simplicity and clarity in system design.\n  * **Experimentation and Observability:** Emphasis on the importance of reproducibility, observability from the outset, and the role of a data registry and model registry in version control.\n\n\nThis structured framework and the insights provided offer a comprehensive view of managing data in LLM systems, helping streamline the development and deployment of AI applications.\nThis is the end of the AI-generated content.\n* * *\n### Abstract\nAny LLM application has four dimensions you must carefully engineer: the code, data, models and prompts. Each dimension influences the other. That's why you must learn how to track and manage each. The trick is that every dimension has particularities requiring unique strategies and tooling. That's why directly applying SWE and DevOps principles that apply to the code does not work for others.\nThis presentation will dig into the data dimension and how it looks when building LLM applications. We will start by exploring a general framework that acts as the foundation. We will look into how the data flows, focusing on the data and features pipeline and how the data should be stored to be correctly shared, versioned, processed and analyzed for RAG, training and inference. Next, we will zoom into how the data is accessed during LLM fine-tuning and within the inference pipeline, which can be implemented as an RAG workflow or as something more complex, such as an agent.\nTo fully understand how the framework works (for building LLM applications), we will look at two concrete use cases, architecting the data layer of the following systems:\n  * An LLM Twin: Your Digital AI Replica (Use case used in our LLM Engineer's Handbook)\n  * A Second Brain AI Assistant (Use case used in our latest open-source course, freely available on Decoding ML)\n\n\nWe will present specific implementation details, tooling, and problems during these two use cases to fully understand an LLM system's data-related generalities and particularities.  \n\n## Interview:\n### What is the focus of your work?\nI am actively working and building LLM, RAG, and informational retrieval systems.\n### What\u2019s the motivation for your talk?\nI want to show people a framework for designing the data layer of RAG and LLM systems using MLOps/LLMOps best practices.\n### Who is your talk for?\nSoftware/ML/AI/Data engineers or data scientists.\n### What do you want someone to walk away with from your presentation?\nArchitect the data layer of an LLM/RAG system\n### What do you think is the next big disruption in software?\nWorkflows and agents\n* * *\n### Speaker\n#### Paul Iusztin\nSenior ML/AI Engineer, MLOps, Founder @Decoding ML\nPaul Iusztin is a senior AI/ML engineer with over seven years of experience building GenAI, Computer Vision and MLOps solutions. His latest contribution was at Metaphysic, where he was one of the core AI engineers who took large GPU-heavy models to production. He previously worked at CoreAI, Everseen, and Continental.\nHe is the co-author of the LLM Engineer's Handbook, a bestseller on Amazon, which presents a hands-on framework for building LLM applications.\nPaul is the Founder of Decoding ML, an educational channel on production-grade AI that provides code, posts, articles, and courses, inspiring others to build real-world AI systems. Through Decoding ML, he collaborated with companies such as MongoDB, Comet, Qdrant, ZenML and 11 other AI companies. \nConnect with him on LinkedIn.\nSubscribe to Decoding ML for weekly content on AI.\nRead more\n#####  Find Paul Iusztin at: \n  * \n\n#### Speaker\n##### Paul Iusztin\nSenior ML/AI Engineer, MLOps, Founder @Decoding ML\n#### Date\nWednesday Apr 9 / 02:45PM BST ( 50 minutes )\n#### Location\nMountbatten (6th Fl.)\n#### Track\nModern Data Architectures\n#### Topics\nAI/ML architecture Generative AI LLM LLMOps\n#### Slides\nSlides are not available\n#### Share\nShare Share\n## From the same track\nSession Data Architecture\n### Reliable Data Flows and Scalable Platforms: Tackling Key Data Challenges\nWednesday Apr 9 / 10:35AM BST\nThere are a few common and mostly well-known challenges when architecting for data. For example, many data teams struggle to move data in a stable and reliable way from operational systems to analytics systems. \nMatthias Niehoff\nHead of Data and Data Architecture @codecentric AG, iSAQB Certified Professional for Software Architecture\nReliable Data Flows and Scalable Platforms: Tackling Key Data Challenges\nSession AI/ML\n### Achieving Precision in AI: Retrieving the Right Data Using AI Agents\nWednesday Apr 9 / 11:45AM BST\nIn the race to harness the power of generative AI, organizations are discovering a hidden challenge: precision. \nAdi Polak\nDirector, Advocacy and Developer Experience Engineering @Confluent, Author of \"Scaling Machine Learning with Spark\" and \"High Performance Spark 2nd Edition\"\nAchieving Precision in AI: Retrieving the Right Data Using AI Agents\nSession Data Architecture\n### Beyond the Warehouse: Why BigQuery Alone Won\u2019t Solve Your Data Problems\nWednesday Apr 9 / 03:55PM BST\nMany organizations mistake the adoption of a data warehouse, like BigQuery, as the golden ticket to solving all their data challenges. But without a robust data strategy and architecture, you\u2019re simply shifting chaos into the cloud. \nSarah Usher\nData & Backend Engineer, Community Director, Mentor\nBeyond the Warehouse: Why BigQuery Alone Won\u2019t Solve Your Data Problems\nSession\n### Panel: Modern Data Architectures\nWednesday Apr 9 / 01:35PM BST\nPanel: Modern Data Architectures\n"
}